{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Notebook\n",
    "\n",
    "The purpose of this notebook is to process and aggregate the *.csv* files provided by `twitch_stream_bot.py` and retrieve the streamer information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T07:29:34.027476Z",
     "iopub.status.busy": "2023-01-14T07:29:34.026953Z",
     "iopub.status.idle": "2023-01-14T07:29:34.208368Z",
     "shell.execute_reply": "2023-01-14T07:29:34.207848Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating streams\n",
    "\n",
    "Twitch's API has changed (end of 2022) and now provides a list of tags and not just a list of tag IDs for each stream.\n",
    "\n",
    "As a result, the stream filtering process in `twitch_stream_bot.py` now checks if a stream has the Vtuber tag id or if it has the word `'Vtuber'` in its tag list. Because of this API enhancement, we are receiving many more new streams than before and because of this, the data collected before and after the change in the filtering process is not comparable.\n",
    "\n",
    "So I decided to split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T07:29:34.210758Z",
     "iopub.status.busy": "2023-01-14T07:29:34.210327Z",
     "iopub.status.idle": "2023-01-14T07:29:34.213024Z",
     "shell.execute_reply": "2023-01-14T07:29:34.212418Z"
    }
   },
   "outputs": [],
   "source": [
    "processing_new_twitch_API_streams = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the *.csv* files are located in the `sync` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T07:29:34.214769Z",
     "iopub.status.busy": "2023-01-14T07:29:34.214505Z",
     "iopub.status.idle": "2023-01-14T07:29:34.409248Z",
     "shell.execute_reply": "2023-01-14T07:29:34.408572Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 68 duplicates\n",
      "Deleting duplicates...\n",
      "There are 0 duplicates\n"
     ]
    }
   ],
   "source": [
    "if processing_new_twitch_API_streams:\n",
    "    csv_path = '../Data/scattered/tagIds_tags/' # For new Twitch API\n",
    "else:\n",
    "    csv_path = '../Data/scattered/tagIds/' # For old Twitch API\n",
    "\n",
    "csv_dfs = [pd.read_csv(csv_path + file) for file in os.listdir(path=csv_path) if file.endswith('csv')]\n",
    "\n",
    "df = pd.concat(csv_dfs, ignore_index=True)\n",
    "\n",
    "print(f'There are {len(df) - len(df.id.drop_duplicates())} duplicates')\n",
    "\n",
    "print('Deleting duplicates...')\n",
    "df.sort_values('_custom_ended_at', inplace=True)\n",
    "df = df.groupby(['id']).apply(lambda df_ : df_.iloc[-1]).reset_index(drop=True)\n",
    "\n",
    "print(f'There are {len(df) - len(df.id.drop_duplicates())} duplicates')\n",
    "\n",
    "df.sort_values('started_at', inplace=True, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing some infos about the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T07:29:34.429210Z",
     "iopub.status.busy": "2023-01-14T07:29:34.428857Z",
     "iopub.status.idle": "2023-01-14T07:29:34.440528Z",
     "shell.execute_reply": "2023-01-14T07:29:34.440031Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4612 streams\n",
      "1006 different broadcasters\n",
      "first started live : 2023-01-05T13:18:16Z\n",
      "last started live : 2023-01-17T04:57:30Z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''{len(df)} streams\n",
    "{len(df.groupby('user_id'))} different broadcasters\n",
    "first started live : {df.iloc[0]['started_at']}\n",
    "last started live : {df.iloc[-1]['started_at']}\n",
    "''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving the dataframe in the `../Data` folder for further use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-14T07:29:34.442595Z",
     "iopub.status.busy": "2023-01-14T07:29:34.442338Z",
     "iopub.status.idle": "2023-01-14T07:29:34.461822Z",
     "shell.execute_reply": "2023-01-14T07:29:34.461226Z"
    }
   },
   "outputs": [],
   "source": [
    "if processing_new_twitch_API_streams:\n",
    "    df.to_csv('../Data/aggregated/twitch_streams_vtuber_tagIds_tags.csv', index=False)\n",
    "else:\n",
    "    df.to_csv('../Data/aggregated/twitch_streams_vtuber_tagIds.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregating videos\n",
    "\n",
    "Quite the same work but with videos... and with more attention on the use of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 6026 id duplicates\n",
      "Deleting duplicates...\n",
      "There are 0 id duplicates\n"
     ]
    }
   ],
   "source": [
    "csv_path = '../Data/scattered/videos/'\n",
    "\n",
    "csv_dfs = [pd.read_csv(csv_path + file) for file in os.listdir(path=csv_path) if file.endswith('csv')]\n",
    "\n",
    "df = pd.concat(csv_dfs, ignore_index=True)\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in os.listdir(path=csv_path):\n",
    "    if file.endswith('csv'):\n",
    "        videos = pd.read_csv(csv_path + file).sort_values('view_count')\n",
    "        df = pd.concat([df, videos]).drop_duplicates()\n",
    "\n",
    "print(f'There are {len(df) - len(df.id.drop_duplicates())} id duplicates')\n",
    "\n",
    "print('Deleting duplicates...')\n",
    "df.sort_values('view_count', inplace=True)\n",
    "df = df.groupby(['id']).apply(lambda df_ : df_.iloc[-1]).reset_index(drop=True)\n",
    "\n",
    "print(f'There are {len(df) - len(df.id.drop_duplicates())} id duplicates')\n",
    "\n",
    "df.sort_values('created_at', inplace=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31859 videos\n",
      "1022 different broadcasters\n",
      "first created video : 2013-02-05T13:47:00Z\n",
      "last created video : 2023-01-17T05:09:49Z\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'''{len(df)} videos\n",
    "{len(df.groupby('user_id'))} different broadcasters\n",
    "first created video : {df.iloc[0]['created_at']}\n",
    "last created video : {df.iloc[-1]['created_at']}\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../Data/aggregated/twitch_videos_vtuber.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying users info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    " # The .json with the CLIENT_ID and the TOKEN\n",
    "with open('../twitch_credentials.json', mode='r') as f:\n",
    "    twitch_credentials = json.load(f)\n",
    "\n",
    "    # Twitch API variables\n",
    "    CLIENT_ID = twitch_credentials['CLIENT_ID']\n",
    "    TOKEN = twitch_credentials['TOKEN']\n",
    "    \n",
    "headers = {\"Authorization\":\"Bearer \" + TOKEN, \"Client-Id\":CLIENT_ID}\n",
    "\n",
    "oldAPIStreams = pd.read_csv('../Data/aggregated/twitch_streams_vtuber_tagIds.csv')\n",
    "newAPIStreams = pd.read_csv('../Data/aggregated/twitch_streams_vtuber_tagIds_tags.csv')\n",
    "\n",
    "streams = pd.concat([oldAPIStreams, newAPIStreams], ignore_index=True)\n",
    "users_id = streams.loc[:, ['user_id']].drop_duplicates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use 2 requests : `Get Users` to get basic user information and `Get Channel Information` to get the channel *tags*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_id_list = list(users_id.user_id.drop_duplicates())\n",
    "\n",
    "channels_descr = pd.DataFrame()\n",
    "users_descr = pd.DataFrame()\n",
    "\n",
    "for i in range(0, len(users_id_list), 100):\n",
    "    url = 'https://api.twitch.tv/helix/channels?broadcaster_id=' + \"&broadcaster_id=\".join(map(str, users_id_list[i:i+100]))\n",
    "    response = requests.get(url, headers=headers)\n",
    "    channels_descr = pd.concat([pd.DataFrame(response.json()[\"data\"]), channels_descr])\n",
    "\n",
    "for i in range(0, len(users_id_list), 100):\n",
    "    url = 'https://api.twitch.tv/helix/users?id=' + \"&id=\".join(map(str, users_id_list[i:i+100]))\n",
    "    response = requests.get(url, headers=headers)\n",
    "    users_descr = pd.concat([pd.DataFrame(response.json()[\"data\"]), users_descr])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We merge the 2 DataFrames to obtain only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels_descr_relevant = channels_descr.loc[:,['broadcaster_id', 'broadcaster_login', 'tags']].rename(columns={'broadcaster_id':'id','broadcaster_login':'login', 'tags':'_channel_tags'})\n",
    "users_descr_tags = pd.merge(users_descr, channels_descr_relevant, how='outer', on=['id','login'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_descr_tags.to_csv('../Data/aggregated/twitch_users_channelTags_vtuber.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(users_descr_tags.iloc[0]._channel_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vtuber",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f2618a5e3146abdf5abc55d1ebd82c144e154d86aee8362b53dc0ec06a76a15"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
